{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for the Wrangle and Analyze Data project at [Udacity](https://www.udacity.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Requirements:](https://review.udacity.com/#!/rubrics/1136/view)\n",
    "- Create a **300-600 word written report** called `wrangle_report.pdf` or `wrangle_report.html` that briefly describes the wrangling efforts. It is framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductory steps that were done\n",
    "- The necessary libraries `pandas`, `numpy`, `requests`, `tweepy`, `json` and `matplotlib.pyplot` imported \n",
    "- The 'magic' command `%matplotlib inline` is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The file `twitter_archive_enhanced.csv` was downloaded from the [link](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv) that was provided by Udacity.\n",
    "\n",
    "### 2. The `image_predictions.tsv` file was downloaded programmatically using the [Requests](https://2.python-requests.org//en/master/) library\n",
    "#### The steps that were done:\n",
    "- The `image_predictions_url` was provided by Udacity. It was saved.\n",
    "- The response was obtained and saved using `requests.get()`.\n",
    "- The response was checked using `response.status_code`. It was equal to 200, as it should.\n",
    "- The `image-predictions.tsv` was opened using [`with open() as` statement](https://docs.python.org/3/reference/compound_stmts.html#the-with-statement)\n",
    "- The `OAuthHandler` was imported from `tweepy`.\n",
    "- The `default_timer` was imported from `timeit` - It measures execution time of small code snippets, see https://docs.python.org/3/library/timeit.html .\n",
    "- The Twitter API was queried for each tweet in the Twitter archive and saved as JSON in a text file. \n",
    "- The actual keys and tokens were replaced by the word 'HIDDEN'.\n",
    "- The `twitter-archive-enhanced.csv` was read to a dataframe using `read_csv()`.\n",
    "- The Twitter's API were queried for JSON data for each tweet ID in the Twitter archive using `with`, `try` and `except`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The obtained `tweet_json.txt` file was read line by line \n",
    "**The steps that were done:**\n",
    "- Ann empty list was created.,\n",
    "- The `tweet_json.txt` file was read using `with`, `readline()` in a `for` loop.\n",
    "- The **tweet ID**, **retweet count**, and **favorite count** were read.\n",
    "- The data were stored to the list.\n",
    "- The list was converted into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Requirements:](https://review.udacity.com/#!/rubrics/1136/view)\n",
    "- Detect and document at least **eight (8) quality issues** and **two (2) tidiness issues**.\n",
    "- To meet specifications, the issues that satisfy the Project Motivation (**see the Key Points** header in `wrangle_act` file) must be assessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The steps that were done:**\n",
    "- The Data Frames obtained from the files `twitter-archive-enhanced.csv`, `image_predictions.tsv` and  `tweet_json.txt` were inspected using the `.head()`, `.info()`, `.value_counts()`, `.duplicated()`, `.islower()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality issues that were detected\n",
    "**in twitter-archive-enhanced:**\n",
    "Only original ratings (no retweets) that have images are needed, however we have\n",
    "\n",
    "1. replies: 78 , \n",
    "2. retweets: 181 , \n",
    "3. entries without images = entries without urls = 2356 entries - 2297 expanded_urls = 59.  \n",
    "4. The columns *in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id\tretweeted_status_user_id\tretweeted_status_timestamp* that are not needed.\n",
    "5. The *timestamp* is object, while it should be date and time.\n",
    "6. There are only four sources of images: *Twitter for iPhone*, *Vine - Make a Scene*, *Twitter Web Client*, and *TweetDeck*, which is not seen, because the full url is given.\n",
    "7. Lower case names are not names.\n",
    "\n",
    "**in img_predictions**\n",
    "8. p1, p2, and p3 column names are unclear\n",
    "9. The 543 images may be not dogs\n",
    "- The underscore in the breed names is unnecessary\n",
    "- The 619 breed names are lower case, but 1532-692 = 840 are upper case \n",
    "\n",
    "### Tidiness issues\n",
    "1. All tables can be merged into the new one on 'tweet_id' index \n",
    "\n",
    "**in twitter-archive-enhanced.csv:**\n",
    "2. The dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) is a variable. The \"stage\" should be one column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The findings:\n",
    "1. The manual check of the figures shows that even with all three predictions as not dogs, there might be a dog in the picture, see, for example, the [link](https://pbs.twimg.com/media/DBW35ZsVoAEWZUU.jpg). Therefore, in order to clean this issue, one needs a manual check, or a better prediction algorithm, which is beyond the scope of this project.\n",
    "2. The `retweet count` and `favorite count` are correlated with each other with the correlation coefficient 0.93. However, they are practically independent from `rating numerator` and day of a week. \n",
    "3. A small correlation is observed between the length of the description `text`, `p1_dog`, `p2_dog` and `p1_conf` with the `favorite_count` only. The corresponding correlation coefficient can be rounded to 0.1. \n",
    "4. The most tweets are unclassified: `None` - 1963 tweets, `pupper` - 209, `doggo` - 72, `puppo` - 23, `floofer` - 8.\n",
    "5. The `None` and `pupper` distributions are non-gaussian, with heavy right tale, while `doggo`, `puppo` and `floofer` distributions have too litle entries to define the shape, see figures below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"retweet_count_distributions.png\" width=\"100%\"> <img src=\"favorite_count_distributions.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. The `None` distribution has two peaks. It may indicate that there is a group of unclasified images, which are much more popular than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. The average number of `favorite_count` is several times larger than the `retweet_count` for every dog 'stage', see figures below.\n",
    "<img src=\"retweet_count_boxplot.png\" width=\"45%\"> <img src=\"favorite_count_boxplot.png\" width=\"45%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. The unclassified `None` tweets have similar popularity as `pupper`, while `doggo`, `puppo` and `floofer` are much popular in both the number of retweets and the number of favorite counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
